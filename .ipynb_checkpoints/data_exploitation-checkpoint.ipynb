{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mohammad Mahdi Gheidi\n",
    "\n",
    "98105976\n",
    "\n",
    "Natural Language Processing Course - HW1 \n",
    "\n",
    "Fall 2023 - Dr. Ehsaneddin Asgari\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این نوت‌بوک به بررسی و آنالیز داده‌های آگهی‌های دسته خودروی سواری سایت دیوار خواهیم پرداخت.\n",
    "\n",
    "داده‌های مورد استفاده در این نوتبوک به وسیله اسکریپت نوشته شده پایتون، که کنار این فایل قرار دارد از سایت دیوار کراول شده است.\n",
    "\n",
    "در ابتدا داده‌ها را لود می‌کنیم و داده‌های مربوط به هر شهر را در متغیر‌های مربوط به خودش قرار می‌دهیم.\n",
    "\n",
    "سپس به وسیله ابزار هضم، به پاکسازی داده‌ها و نرمال کردنشان می‌پردازیم.\n",
    "\n",
    "در انتها نیز به بررسی و آنالیز داده‌های مربوط به آگهی‌ها می‌پردازیم."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repos.divar.cloud/artifactory/api/pypi/pypi-public/simple\n",
      "Requirement already satisfied: hazm==0.9.4 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: matplotlib==3.8.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (3.8.0)\n",
      "Requirement already satisfied: nltk==3.8.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: numpy==1.24.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.24.3)\n",
      "Requirement already satisfied: requests==2.31.0 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn==1.3.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: scipy==1.11.3 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.11.3)\n",
      "Requirement already satisfied: tqdm==4.66.1 in ./venv/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (4.66.1)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in ./venv/lib/python3.10/site-packages (from hazm==0.9.4->-r requirements.txt (line 1)) (4.3.2)\n",
      "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in ./venv/lib/python3.10/site-packages (from hazm==0.9.4->-r requirements.txt (line 1)) (0.9.2)\n",
      "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in ./venv/lib/python3.10/site-packages (from hazm==0.9.4->-r requirements.txt (line 1)) (0.9.9)\n",
      "Requirement already satisfied: flashtext<3.0,>=2.7 in ./venv/lib/python3.10/site-packages (from hazm==0.9.4->-r requirements.txt (line 1)) (2.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (3.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (10.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (4.43.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.10/site-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: joblib in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.10/site-packages (from nltk==3.8.1->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 5)) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 5)) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 5)) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests==2.31.0->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./venv/lib/python3.10/site-packages (from scikit-learn==1.3.1->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./venv/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.9.4->-r requirements.txt (line 1)) (59.6.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./venv/lib/python3.10/site-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm==0.9.4->-r requirements.txt (line 1)) (2.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./venv/lib/python3.10/site-packages (from gensim<5.0.0,>=4.3.1->hazm==0.9.4->-r requirements.txt (line 1)) (6.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib==3.8.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import emoji\n",
    "import hazm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sklearn\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'color': 'سفید',\n",
      " 'description': 'خودروی تمیز ✔️\\n'\n",
      "                'رنگ سفید✔️\\n'\n",
      "                'بدون رنگ✔️\\n'\n",
      "                'کم کار کرد ✔️\\n'\n",
      "                'ماشین جوان پسند',\n",
      " 'milage': '۶۱٬۰۰۰',\n",
      " 'model': '۱۴۰۰',\n",
      " 'subtitle': 'دقایقی پیش در تبریز',\n",
      " 'title': 'تیبا صندوق\\u200cدار SX، مدل ۱۴۰۰'}\n",
      "{'color': 'سفید',\n",
      " 'description': 'فقط گلگیر جلو سمت شاگرد تعویض شده با شماره *********** تماس '\n",
      "                'بگیرید',\n",
      " 'milage': '۱۷۰٬۰۰۰',\n",
      " 'model': '۱۳۹۲',\n",
      " 'subtitle': 'دقایقی پیش در ارومیه',\n",
      " 'title': 'رنو پارس تندر دنده\\u200cای، مدل ۱۳۹۲'}\n"
     ]
    }
   ],
   "source": [
    "cities_slugs = ['tehran', 'karaj', 'isfahan', 'tabriz', 'urmia', 'rasht', 'sari', 'sanandaj']\n",
    "data = {}\n",
    "for city_slug in cities_slugs:\n",
    "    with open(f'data/{city_slug}_car_datas.json', 'r') as f:\n",
    "        data[city_slug] = json.load(f)\n",
    "\n",
    "pprint(data['tabriz'][0])\n",
    "pprint(data['urmia'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis_from_data(text):\n",
    "    # A regular expression to identify various types of emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    \n",
    "    # Remove emojis using the regular expression\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_from_data(text):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(text):\n",
    "    # Normalize the text using hazm library\n",
    "    text = hazm.Normalizer().normalize(text)\n",
    "    \n",
    "    # Remove punctuations\n",
    "    # text = re.sub(r'[^آ-ی0-9a-zA-Z ]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_data(text):\n",
    "    # Lemmatize the text using hazm library\n",
    "    return ' '.join(hazm.Lemmatizer().lemmatize(word) for word in hazm.word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    pipeline = [\n",
    "        remove_emojis_from_data,\n",
    "        remove_stop_words_from_data,\n",
    "        normalize_data,\n",
    "        lemmatize_data,\n",
    "    ]\n",
    "    for city_slug in cities_slugs:\n",
    "        for item in data[city_slug]:\n",
    "            for func in pipeline:\n",
    "                item['description'] = func(item['description'])\n",
    "                item['title'] = func(item['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech_tagging():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_suitable_title_from_description():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
